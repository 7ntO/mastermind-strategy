\chapter{Optimal Strategies}

After examining a variety of heuristic strategies that perform variably under different configuration of rules, a natural question arise: for a given set of rules, is there an optimal strategy? The answer is ``yes''. In this chapter, we are going to explore the techniques that enable us to find such a strategy.

\section{Overview}

An \emph{optimal strategy} is a strategy that achieves a certain objective in an optimal manner. Three types of objectives are typical:

1) To minimize the worst-case number of guesses needed to reveal the secret.

2) To minimize the average number of guesses needed to reveal the secret.

3) To minimize the average number of guesses while keeping the worse-case steps to a minimum.

Note that in each case, we could replace ``reveal'' with ``determine'', which are subtly different from an information perspective (see 2.4). However the overall methodology will remain the same. Therefore in the following we will proceed with the above goals and give results to the ``determine'' version along the way.

For the first goal, the \minmax{} heuristic strategy (2.2) already provides an optimal solution, as it reveals all secrets with no more than 5 guesses and any strategy cannot use fewer [proof???]. So in this chapter we will focus on the second and third objectives.

Note that there may be other objectives for a strategy, such as minimizing the number of guesses evaluated. See, for example, Temporel and Kovacs (2003). However, those objectives are not studied in this chapter.

The theory to find an optimal strategy is simple. Since the number of possible secrets as well as sequence of (non-redundant) guesses are finite, the code breaker can employ a depth-first search to find out the optimal guessing strategy.

The optimal strategy minimizes the expected number of guesses, optionally subject to a maximum-guesses constraint. Finding such a strategy involves exhaustive search, and therefore is too slow to be suitable for real-time application. For more details on the implementation, see optimal strategies.

[show the search scale of the problem]

[show that an optimal strategy is not unique]

[Tanaka also noted that a strategy which produces a minimum expected
game length is not the optimal strategy for head-to-head play between two human
opponents when a win by any number of moves scores the same as a win by a
single move.] What does this mean??


\section{Obviously-optimal guesses}

Some definitions. Partitions, feedback count, etc.



While finding an optimal strategy for the general game is complex, in certain cases it's easy. For example, when there's only one possibility left, we should guess it. When there are only two possibilities left, we should guess (either) one of them. [these appear in Neuwirth 82]. When there are more than two possibilities, it's still possible.

An obviously-optimal guess is an optimal guess that doesn't require too much effort to identify. Depending on the techniques used to identify such, the "obvious"-ty could vary. Here we use the technique introduced by \cite{koyama93}. 

[Definition.] An obviously-optimal guess is a guess that partitions the remaining possibilities into discrete cells, i.e.\ where every cell contains exactly one element. 

If such a guess exists and comes from the possibility set, then it is optimal because it reveals one potential secret (itself) in the immediate step and reveals all the other potential secrets in two steps. It is easy to see that no other strategy could do better. If no such guess exists in the possibility set but one exists outside the possibility set, then that one is optimal because it reveals all secrets in two steps. 

Note that an obviously-optimal guess is fairly generic about the goal -- it is optimal both in terms of the worst-case number of steps and the expected number of steps to determine or reveal the secret.

A necessary condition for an obviously-optimal guess to exist is that the number of remaining possibilities does not exceed the number of distinct feedbacks. For a game with $p$ pegs, the number of distinct feedbacks is $p(p+3)/2$. For example, in a four-peg game, there can be at most 14 secrets left for an obviously-optimal guess to exist. This is a useful check in practice to reduce unnecessary efforts to search for an obviously optimal guess.

Note also that in practice we may only want to check in the remaining possibilities (so that the effort is minimized). In turns out that if we check outside the remaining possibilities, it is equivalent to a full-run of a heuristic function, as we show below.

It turns out (not so surprisingly) that the heuristics introduced in the previous chapter will yield an obviously-optimal guess when one exists. We only need to show that the partition of an obviously-optimal guess (which we will call an \emph{obviously optimal partition} and denote by $Q$ below) achieves the lowest possible heuristic value.

Let $P$ denote any given partition. Let $k$ denote the number of (non-empty) cells in $P$. Let $n_i$ denote the number of elements in the $i$-th cell. Let $n = \sum_{i=1}^k$ denote the total number of elements, which is invariant across different $P$. Finally, let $Q$ denote the partition of an obviously-optimal guess, i.e. one with all singleton cells.

\paragraph{Min-Max} 
The \minmax{} heuristic value of an obviously optimal partition is one. This is the minimum value of the heuristic function.
\[
h(P) = \max_{1 \le i \le n} n_i \ge 1 = h(Q).
\]

\paragraph{Min-Avg}
The \minavg{} heuristic value of an obviously optimal partition is one. This is the minimum value of the function.
\[
h(P) = \sum_{i=1}^k \frac{n_i}{n} n_i \ge \min_{1 \le i \le k} n_i \ge 1 = h(Q).
\]

\paragraph{Max-Entropy}
The \maxent{} heuristic value of an obviously optimal partition is ?. This is the maximum value of the function.
\[
h(P) = - \sum_{i=1}^k \frac{n_i}{n} \log \frac{n_i}{n} = ?
\]

\paragraph{Max-Parts}
The \maxpar{} heuristic value of an obviously optimal partition is $n$. This is the maximum value of the function.
\[
h(P) = k \le n = h(Q).
\]

Since all four heuristics introduced yield an obviously optimal guess when one exists, we can insert the step to find an optimal guess into the strategy as a shortcut to save computation time, knowing that this will not alter the output of the heuristic strategy.

\section{Equivalence of guesses}

Just as a human would be easy to identify equivalent guesses, we use that in computer as well.

[]Isomorphism of guesses]

\subsection{Color equivalence}

As a start-up, let's work with a simple but useful way to detect guess equivalence.

\subsection{Constraint equivalence}

Here the definition of \emph{constraint equivalence} is rather restrictive. Ideally, constraint equivalence should be the same as partition equivalence, but this is not the case here.

Introduced by \cite{neuwirth81,koyama93}. 

[Constraint/filter equivalence]

It is easy to see that solving a game only depends on what colors have appeared before, and the relative peg position of the guesses. The particular colors or pegs guessed are not important. From this knowledge, we define the following.

\subsubsection{Definition}

\begin{definition}
(Color permutation) A \emph{color permutation}, denoted $\pi_c$, is a permutation of the colors used in a game. For example, in a game with 6 colors, a color permutation could be
\[
\begin{pmatrix}
1 & 2 & 3 & 4 & 5 & 6 \\
5 & 2 & 1 & 6 & 3 & 4
\end{pmatrix} ,
\]
which means color 1 replaced by color 5, color 2 unchanged, color 3 replaced by color 1, etc.
\end{definition}

\begin{definition}
(Peg permutation) A \emph{peg permutation}, denoted $\pi_p$, is a permutation of the pegs used in a game. For example, in a game with 4 pegs, a peg permutation could be
\[
\begin{pmatrix}
1 & 2 & 3 & 4 \\
3 & 1 & 2 & 4
\end{pmatrix} ,
\]
which means peg 1 moved to the 3rd place, peg 2 moved to the 1st place, peg 3 moved to the 2nd place, and peg 4 remaining in the same place. Note that the reordered sequence of the pegs is represented by $\pi_p^{-1}$, which in the above example is equal to
\[
\begin{pmatrix}
1 & 2 & 3 & 4 \\
2 & 3 & 1 & 4
\end{pmatrix} .
\]
\end{definition}

\begin{definition}
(Codeword permutation) A \emph{codeword permutation}, denoted $\pi$, is the combination of a peg permutation $\pi_p$ and a color permutation $\pi_c$. We write it as $\pi=(\pi_p,\pi_c)$. Note that the actual order of applying the peg permutation or the color permutation first doesn't matter.
\end{definition}

Given codeword $g$, we denote the permuted codeword under $\pi$ as $\pi(g)$ or $g^\pi$. Note that since a codeword usually does not contain all available colors, the mapping of unused colors does not matter when applying a color permutation to a given codeword.

To actually compute the image $h = (h_i)$ of a given codeword $g = (g_i)$ under a given permutation $\pi = (\pi_p, \pi_c)$, use the formula
\[
h_i = \pi_c\left(g_{\pi_p^{-1}(i)}\right) .
\]

\begin{definition}
(Codeword equivalence) Two codewords $g$ and $h$ are \emph{equivalent} if and only if there exists a codeword permutation $\pi=(\pi_p,\pi_c)$ such that $g^\pi = h$.
\end{definition}

[Properties.] Some usual properties of permutations apply to codeword permutation as well.

1) A codeword permutation is invertible, i.e.\ for any $\pi = (\pi_p, \pi_c)$, its inverse exists and is equal to $\pi^{-1} = (\pi_p^{-1}, \pi_c^{-1})$.

2) The equivalence relation defined above is indeed an \emph{equivalence relation}. Hence, in each equivalence class, there exists one \emph{representative}, also called the \emph{canonical element}. All elements in the same equivalence class are equivalent to this element; i.e.\ for all $g$, there exists $\pi$ such that $g = g_0^\pi$.

\begin{definition}
(Constraint equivalence) (see a few references) Let $\mathcal{C}_1$ and $\mathcal{C}_2$ be two ordered sets of constraints of the same size $r$. Let the guesses in $\mathcal{C}_1$ be $(g_1,g_2,\cdots,g_r)$ and the guesses in $\mathcal{C}_2$ be $(h_1,h_2,\cdots,h_r)$. Then constraint sets $\mathcal{C}_1$ and $\mathcal{C}_2$ are equivalent if and only if there exists a codeword permutation $\pi=(\pi_p,\pi_c)$ such that
$g_i^\pi = h_i$ for $1 \le i \le r$.
\end{definition}

The intuition about equivalent constraints are rather simple: if we rearrange the peg positions and relabel the colors, then the two sets of constraints become exactly the same. Hence for the purpose of devising a strategy, we only need to work with one of them and the other follows automatically.

Note, however, that there is a strong restriction in the above definition: the feedback in the constraints are ignored. This has two implications. On the one hand, constraints such as 
\[
\mathcal{C}_1 = \{ (1234:0A0B), (3456:2A0B) \}
\]
and
\[
\mathcal{C}_2 = \{ (1234:1A0B), (3456:0A2B) \}
\]
are considered equivalent, although they lead to drastically different potential secrets. On the other hand, constraints such as
\[
\mathcal{C}_1 = \{ (1234:0A0B), (1234:0A0B) \}
\]
and
\[
\mathcal{C}_2 = \{ (1234:0A0B), (4321:0A0B) \}
\]
are obviously not equivalent by definition, but from the first feedback \fb{0}{0} we could already exclude the colors \cw{1} -- \cw{4} from the potential secret, so any permutation of \cw{1234} should ideally be considered equivalent.

Despite the less-than-ideal properties of such definition, this equivalence relation is still commonly used \cite{neuwirth81,koyama93,francis10} because of its clarity, relative simplicity, and effectiveness for the first few guesses. A more powerful (but also more complex) equivalence definition will be introduced in the next section.

To actually apply a constraint equivalence filter in practice, we implement an \emph{incremental filter}, as described below.\footnote{The incremental filter described in this section is effectively the same technique used by \cite{francis10} to detect equivalence in the Bulls and cows game.}
An alternative, more generic method, which relies on \emph{graph automorphism}, will be introduced in the next section.

Given a set $\mathcal{C}$ of constraints and a set $\mathcal{S}$ of candidate codewords for the next guess, we want to find all canonical codewords $g \in \mathcal{S}$ to be used as the next guess. There are a couple of ways to do this, and we employ a simple algorithm: traverse the set and keep the codewords that are lexically minimal in its equivalence class. That is, for each $g \in \mathcal{S}$, we find the lexically minimum codeword $g_0$ that is equivalent to $g$, and keep $g$ if and only if $g = g_0$.

Note that in order for the algorithm to work correctly, the lexically minimal codeword of each equivalence class \emph{must} be present in the candidate set $\mathcal{S}$.

To check whether $g$ is lexically minimal in its equivalence class, we iterate all permutations of $g$ to see if any permutation is lexically smaller. 

We make the assumption that the number of colors is more than the number of pegs, which is typical in a real-world game.

Suppose there are $p$ pegs. We first list all $p!$ permutations of the pegs. For example, in a 4-peg game, there are 24 peg permutations, $\pi_p^1$ to $\pi_p^{24}$, where $\pi_p^1$ is the identity permutation.

Then, for each peg permutation $\pi_p^i$, we associate with it a \emph{partial permutation} of the colors, $\Pi_c^i$. A partial permutation is an incomplete permutation where the mapping of some colors are fixed, but the mapping the rest colors can be altered flexibly. As will be shown shortly, a partial permutation in this context is actually a \emph{permutation group}. [put the definition in the appendix]

At the beginning of the game, there are no constraints, hence all color mappings are flexible. We can conveniently write them as
\[
\Pi_c^i = 
\begin{pmatrix}
1^* & 2^* & 3^* & 4^* & 5^* & 6^*  \\
1^* & 2^* & 3^* & 4^* & 5^* & 6^* 
\end{pmatrix} ,
\]
where an asterisk indicates a pair of mapping that is flexible to be altered.

To filter a set of codewords $\mathcal{S}$ to obtain canonical guesses, we check each codeword $g \in \mathcal{S}$ in order.\footnote{The particular order of traversal is not important, as long as the lexical minimum belongs to the set.} 
Take for example $g = \cw{1223}$. We then iterate through each peg permutation $\pi_p^i$ to permute it. Apply for example the following permutation,
\[
\pi_p = 
\begin{pmatrix}
1 & 2 & 3 & 4 \\
2 & 1 & 3 & 4
\end{pmatrix} ,
%\]
%whose associated partial color permutation is initially
%\[
\Pi_c = 
\begin{pmatrix}
1^* & 2^* & 3^* & 4^* & 5^* & 6^*  \\
1^* & 2^* & 3^* & 4^* & 5^* & 6^* 
\end{pmatrix} .
\]
The permuted codeword is $g' = \cw{2123}$. Since $g' \succ g$, we cannot yet conclude that $g$ is not the lexical minimum. We proceed as follows.

Since the color permutation is fully flexible, we are free to map the colors in $g'$ freely to any color. Given our objective is to find the lexical minimum, we start from the leftmost peg of $g'$, which contains the color \cw{2}. This color should be mapped to be smallest free color (\cw{1}) to achieve lexical minimum, which also implies we have to map color \cw{1} to something else temporarily because we cannot have two colors map to the same value. For convenience we map it to \cw{2}. Thus we apply the following color permutation to $g' = \cw{2123}$,
\[
\pi_c = 
\begin{pmatrix}
1^* & 2 & 3^* & 4^* & 5^* & 6^*  \\
2^* & 1 & 3^* & 4^* & 5^* & 6^* 
\end{pmatrix} ,
\]
which yields $\pi_c(g') = \cw{1213}$.

Note that \cw{1213} is already lexically smaller than \cw{1223}, so we can conclude that \cw{1223} is not the lexical minimum of its equivalence class and can thus remove it from the candidates. However, for illustration purpose here we show a few more steps.

Now proceed to the color on the second peg of $g'$, \cw{1}. Again, \cw{1} is not mapped in the color permutation, so we are free to choose its image. And for the same reason of achieving minimal lexicographical value, we map it to the smallest available color, which in this case is \cw{2}. The resulting color permutation is
\[
\pi_c = 
\begin{pmatrix}
1 & 2 & 3^* & 4^* & 5^* & 6^*  \\
2 & 1 & 3^* & 4^* & 5^* & 6^* 
\end{pmatrix} ,
\]
and the permuted codeword is $\pi_c(g') = \cw{1213}$. Repeat this until all pegs in $g'$ are processed, and we find the lexical minimum to be \cw{1213}, which is smaller than $g$. We then remove $g$ from the candidate set. 

Repeat the above steps for all peg permutations. If $g$ is lexically minimum in all these cases, then it's good to add to the canonical set.

Above is an examine where all color permutations are fully flexible. Now suppose we have already have some constraints. Without loss of generality, suppose that the constraint consists of just one guess $g_1 = \cw{3445}$. To find all canonical candidates for the second guess, we first need to restrict the partial color permutations associated with each peg permutation. 

Take for example 
\[
\pi_p^3 = 
\begin{pmatrix}
1 & 2 & 3 & 4 \\
1 & 3 & 2 & 4
\end{pmatrix} .
\] 
The peg-permuted first guess is $\pi_p^3(g_1) = \cw{3445}$. In order for the permuted constraint to be equivalent, the permuted guess must be equal to $g_1$. This means we must map \cw{3445} to \cw{3445} in the color permutation. Thus we must restrict the associated color permutation as
\[
\Pi_c^3 = 
\begin{pmatrix}
1 & 2 & 3 & 4 & 5 & 6 \\
* & * & 3 & 4 & 5 & * 
\end{pmatrix} .
\]

After restricting the partial permutation, we can apply the previous steps again. Suppose we come again to \cw{1123}. If we apply peg permutation $\pi_p^3$, the permuted representative becomes \cw{1213}. Now to find out what color permutations are eligible, note that the color 3 is already fixed in the partial permutation; only colors 1 and 2 are free to be mapped. We could therefore map them freely among the unrestricted colors, i.e.\ $\{1, 2, 6\}$, and mark all resulting codewords as equivalent to \cw{1123}.

Note that given a set of constraints, not all peg permutations can yield an eligible color permutation. For example, consider the peg permutation
\[
\pi_p^2 = 
\begin{pmatrix}
1 & 2 & 3 & 4 \\
1 & 2 & 4 & 3
\end{pmatrix} .
\]
When applied to $g_1 = \cw{3445}$, we get $\pi_p^2(g_1) = \cw{3454}$. However, there is no way whatsoever to map \cw{3445} to \cw{3454}, because 4 would have to be mapped to both 4 and 5, which is impossible. In this case, we call the peg permutation ($\pi_p^2$) \emph{ineligible} and remove it from further consideration.

Formally, let $\mathcal{F} = \left\{(\pi_p^i, \Pi_c^i) \given 1 \le i \le m \right\}$ be an incremental equivalence filter containing $m$ (partial) codeword permutations. Each time a constraint (with guess $g$) is added, we restrict the partial color permutation by requiring $\pi_c^i ( \pi_p^i (g) ) = \pi_p^i (g)$ for all $\pi_c^i \in \Pi_c^i$. 

It can be seen from the above requirement that any color that is mapped from in a partial color partition is also mapped to, and any color that is not mapped in one direction is also not mapped in the other direction. In fact, the unmapped colors are those \emph{unused} in any of the prior constraints. This means this equivalence filter fully considers the ``unguessed color equivalence'' described in the previous section.

As we proceed in the game, we are supplied with more constraints. The partial color permutation associated with each peg permutation gets more restrictive with each
added constraint, and more peg permutations becomes ineligible and gets removed. Finally we will be left with only the identity codeword permutation, where every codeword will become a representative of its own equivalence class.
%Hence we call it "incremental equivalence detection".

Applying the above constraint equivalence filter, we find 5 canonical guesses for the first round of a standard Mastermind game (p4c6r), listed in Table \ref{tab:canonical-mastermind}. Listed alongside is the number of canonical guesses in the second round given the initial guess.
\begin{table}[h]
\begin{center}
\begin{tabular}{c c}
\hline
\hline
$g_1$ & $\#\{g_2\}$ \\
\hline
\cw{0000} & 12 \\
\cw{0001} & 53 \\
\cw{0011} & 39 \\
\cw{0012} & 130 \\
\cw{0123} & 57 \\
\hline
\hline
\end{tabular}
\caption{Canonical 1st and 2nd guesses in Mastermind}
\label{tab:canonical-mastermind}
\end{center}
\end{table}

Applying the same filter to the Bulls and cows game (p4c10n) yields only one canonical initial guess, \cw{0123}, and 20 canonical guesses for the second round. They are listed in Table \ref{tab:canonical-bulls} along with the number of canonical guesses for the third round.\footnote{The table on page 7 of \cite{francis10} contains the same information; however their number for \cw{0456} is 373 and their number for \cw{4567} is 218.}
\begin{table}[h]
\begin{center}
\begin{tabular}{c c | c c | c c | c c}
\hline
\hline
$g_2$ & $\#\{g_3\}$ & $g_2$ & $\#\{g_3\}$ & $g_2$ & $\#\{g_3\}$ & $g_2$ & $\#\{g_3\}$ \\
\hline
\cw{0123} & 20  & \cw{0214} & 270  & \cw{1032} & 39  & \cw{1234} & 501  \\
\cw{0124} & 107 & \cw{0231} & 75   & \cw{1034} & 270 & \cw{1245} & 1045 \\
\cw{0132} & 67  & \cw{0234} & 501  & \cw{1045} & 295 & \cw{1435} & 541  \\
\cw{0134} & 270 & \cw{0245} & 1045 & \cw{1204} & 175 & \cw{1456} & 1012 \\
\cw{0145} & 295 & \cw{0456} & 363  & \cw{1230} & 59  & \cw{4567} & 180  \\
\hline
\hline
\end{tabular}
\caption{Canonical 2nd and 3rd guesses in Bulls and cows}
\label{tab:canonical-bulls}
\end{center}
\end{table}

[consistent with all prior guesses]

\subsection{Partition equivalence}

State/Partition Equivalence

\section{Search space pruning}

\section{Other techniques}

(e.g. two-phase optimization, hash collision group)

\section{Using a pre-built strategy tree}

\section{Extended/Adaptive strategy tree}

i.e. the tree not only contains guesses along the chosen strategy path, but also includes guesses if the user made a non-optimal guess halfway. The tree size in this case is much larger, and we must use isomorphism to detect the symmetry.

 
