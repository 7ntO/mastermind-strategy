\section{Optimal Strategies}

After examining a variety of heuristic strategies that perform variably under different configuration of rules, a natural question arise: for a given set of rules, is there an optimal strategy? The answer is ``yes''. In this chapter, we are going to explore the techniques that enable us to find such a strategy.

\subsection{Overview}

An \emph{optimal strategy} is a strategy that achieves a certain objective in an optimal manner. Three types of objectives are typical:

1) To minimize the worst-case number of guesses needed to reveal the secret.

2) To minimize the average number of guesses needed to reveal the secret.

3) To minimize the average number of guesses while keeping the worse-case steps to a minimum.

Note that in each case, we could replace ``reveal'' with ``determine'', which are subtly different from an information perspective (see 2.4). However the overall methodology will remain the same. Therefore in the following we will proceed with the above goals and give results to the ``determine'' version along the way.

For the first goal, the \minmax{} heuristic strategy (2.2) already provides an optimal solution, as it reveals all secrets with no more than 5 guesses and any strategy cannot use fewer [proof???]. So in this chapter we will focus on the second and third objectives.

Note that there may be other objectives for a strategy, such as minimizing the number of guesses evaluated. See, for example, Temporel and Kovacs (2003). However, those objectives are not studied in this chapter.

The theory to find an optimal strategy is simple. Since the number of possible secrets as well as sequence of (non-redundant) guesses are finite, the code breaker can employ a depth-first search to find out the optimal guessing strategy.

The optimal strategy minimizes the expected number of guesses, optionally subject to a maximum-guesses constraint. Finding such a strategy involves exhaustive search, and therefore is too slow to be suitable for real-time application. For more details on the implementation, see optimal strategies.

[show the search scale of the problem]

[show that an optimal strategy is not unique]

[Tanaka also noted that a strategy which produces a minimum expected
game length is not the optimal strategy for head-to-head play between two human
opponents when a win by any number of moves scores the same as a win by a
single move.] What does this mean??


\subsection{Obviously-optimal guesses}

Some definitions. Partitions, feedback count, etc.



While finding an optimal strategy for the general game is complex, in certain cases it's easy. For example, when there's only one possibility left, we should guess it. When there are only two possibilities left, we should guess (either) one of them. [these appear in Neuwirth 82]. When there are more than two possibilities, it's still possible.

An obviously-optimal guess is an optimal guess that doesn't require too much effort to identify. Depending on the techniques used to identify such, the "obvious"-ty could vary. Here we use the technique introduced by \cite{koyama93}. 

[Definition.] An obviously-optimal guess is a guess that partitions the remaining possibilities into discrete cells, i.e.\ where every cell contains exactly one element. 

If such a guess exists and comes from the possibility set, then it is optimal because it reveals one potential secret (itself) in the immediate step and reveals all the other potential secrets in two steps. It is easy to see that no other strategy could do better. If no such guess exists in the possibility set but one exists outside the possibility set, then that one is optimal because it reveals all secrets in two steps. 

Note that an obviously-optimal guess is fairly generic about the goal -- it is optimal both in terms of the worst-case number of steps and the expected number of steps to determine or reveal the secret.

A necessary condition for an obviously-optimal guess to exist is that the number of remaining possibilities does not exceed the number of distinct feedbacks. For a game with $p$ pegs, the number of distinct feedbacks is $p(p+3)/2$. For example, in a four-peg game, there can be at most 14 secrets left for an obviously-optimal guess to exist. This is a useful check in practice to reduce unnecessary efforts to search for an obviously optimal guess.

Note also that in practice we may only want to check in the remaining possibilities (so that the effort is minimized). In turns out that if we check outside the remaining possibilities, it is equivalent to a full-run of a heuristic function, as we show below.

It turns out (not so surprisingly) that the heuristics introduced in the previous chapter will yield an obviously-optimal guess when one exists. We only need to show that the partition of an obviously-optimal guess (which we will call an \emph{obviously optimal partition} and denote by $Q$ below) achieves the lowest possible heuristic value.

Let $P$ denote any given partition. Let $k$ denote the number of (non-empty) cells in $P$. Let $n_i$ denote the number of elements in the $i$-th cell. Let $n = \sum_{i=1}^k$ denote the total number of elements, which is invariant across different $P$. Finally, let $Q$ denote the partition of an obviously-optimal guess, i.e. one with all singleton cells.

\paragraph{Min-Max} 
The \minmax{} heuristic value of an obviously optimal partition is one. This is the minimum value of the heuristic function.
\[
h(P) = \max_{1 \le i \le n} n_i \ge 1 = h(Q).
\]

\paragraph{Min-Avg}
The \minavg{} heuristic value of an obviously optimal partition is one. This is the minimum value of the function.
\[
h(P) = \sum_{i=1}^k \frac{n_i}{n} n_i \ge \min_{1 \le i \le k} n_i \ge 1 = h(Q).
\]

\paragraph{Max-Entropy}
The \maxent{} heuristic value of an obviously optimal partition is ?. This is the maximum value of the function.
\[
h(P) = - \sum_{i=1}^k \frac{n_i}{n} \log \frac{n_i}{n} = ?
\]

\paragraph{Max-Parts}
The \maxpar{} heuristic value of an obviously optimal partition is $n$. This is the maximum value of the function.
\[
h(P) = k \le n = h(Q).
\]

Since all four heuristics introduced yield an obviously optimal guess when one exists, we can insert the step to find an optimal guess into the strategy as a shortcut to save computation time, knowing that this will not alter the output of the heuristic strategy.

\subsection{Equivalence of guesses}

Just as a human would be easy to identify equivalent guesses, we use that in computer as well.

[]Isomorphism of guesses]

\subsubsection{Color equivalence}

As a start-up, let's work with a simple but useful way to detect guess equivalence.

\subsubsection{Constraint equivalence}

Here the definition of \emph{constraint equivalence} is rather restrictive. Ideally, constraint equivalence should be the same as partition equivalence, but this is not the case here.

Introduced by \cite{neuwirth81,koyama93}. 

[Constraint/filter equivalence]

It is easy to see that solving a game only depends on what colors have appeared before, and the relative peg position of the guesses. The particular colors or pegs guessed are not important. From this knowledge, we define the following.

\begin{definition}
(Color permutation) A \emph{color permutation}, denoted $\pi_c$, is a permutation of the colors used in a game. For example, in a game with 6 colors, a color permutation could be
\[
\begin{pmatrix}
1 & 2 & 3 & 4 & 5 & 6 \\
5 & 2 & 1 & 6 & 3 & 4
\end{pmatrix} ,
\]
which means color 1 replaced by color 5, color 2 unchanged, color 3 replaced by color 1, etc.
\end{definition}

\begin{definition}
(Peg permutation) A \emph{peg permutation}, denoted $\pi_p$, is a permutation of the pegs used in a game. For example, in a game with 4 pegs, a peg permutation could be
\[
\begin{pmatrix}
1 & 2 & 3 & 4 \\
3 & 1 & 2 & 4
\end{pmatrix} ,
\]
which means the color originally associated with peg 1 is now associated with peg 3, the color originally associated with peg 2 is now associated with peg 1, etc.
\end{definition}

A somewhat confusing property of a peg permutation is that the second row in the notation, $(3 1 2 4)$, does \emph{not} represent the reordered sequence of the original pegs. The actual sequence of the original pegs after the permutation is $\pi_p^{-1}$, i.e.
\[
\begin{pmatrix}
1 & 2 & 3 & 4 \\
2 & 3 & 1 & 4
\end{pmatrix} .
\]

\begin{definition}
(Codeword permutation) A \emph{codeword permutation}, denoted $\pi$, is the combination of a peg permutation $\pi_p$ and a color permutation $\pi_c$. We write it as $\pi=(\pi_p,\pi_c)$. Note that the actual order of applying the peg permutation first or the color permutation first doesn't matter.
\end{definition}

Given codeword $g$, we denote the permuted codeword under $\pi$ as $\pi(g)$ or $g^\pi$. Note that since a codeword usually does not contain all available colors, the mapping of unused colors does not matter when applying a color permutation to a given codeword.

To actually compute the image $h = (h_i)$ of a given codeword $g = (g_i)$ under a given permutation $\pi = (\pi_p, \pi_c)$, use the formula
\[
h_i = \pi_c\left(g_{\pi_p^{-1}(i)}\right) .
\]

\begin{definition}
(Codeword equivalence) Two codewords $g$ and $h$ are \emph{equivalent} if and only if there exists a codeword permutation $\pi=(\pi_p,\pi_c)$ such that $g^\pi = h$.
\end{definition}

[Properties.] Some usual properties of permutations apply to codeword permutation as well.

1) A codeword permutation is invertible, i.e.\ for any $\pi$, there exists $\pi^{-1}$.

2) The equivalence relation defined above is indeed an \emph{equivalence relation}. Hence, in each equivalence class, there exists one \emph{representative}, also called the \emph{canonical element}. All elements in the same equivalence class are equivalent to this element; i.e.\ for all $g$, there exists $\pi$ such that $g = g_0^\pi$.

\begin{definition}
(Constraint equivalence) (see a few references) Let $\mathcal{C}_1$ and $\mathcal{C}_2$ be two ordered sets of constraints of the same size $r$. Let the guesses in $\mathcal{C}_1$ be $(g_1,g_2,\cdots,g_r)$ and the guesses in $\mathcal{C}_2$ be $(h_1,h_2,\cdots,h_r)$. Then constraint sets $\mathcal{C}_1$ and $\mathcal{C}_2$ are equivalent if and only if there exists a codeword permutation $\pi=(\pi_p,\pi_c)$ such that
$g_i^\pi = h_i$ for $1 \le i \le r$.
\end{definition}

The intuition about equivalent constraints are rather simple: if we rearrange the peg positions and relabel the colors, then the two sets of constraints become exactly the same. Hence for the purpose of devising a strategy, we only need to work with one of them and the other follows automatically.

Note, however, that there is a critical restriction in the above definition: the feedback in the constraints are ignored. This has two implications. On the one hand, constraints such as 
\[
\mathcal{C}_1 = \{ (1234:0A0B), (3456:2A0B) \}
\]
and
\[
\mathcal{C}_2 = \{ (1234:1A0B), (3456:0A2B) \}
\]
are considered equivalent, although they lead to drastically different potential secrets. On the other hand, constraints such as
\[
\mathcal{C}_1 = \{ (1234:0A0B), (1234:0A0B) \}
\]
and
\[
\mathcal{C}_2 = \{ (1234:0A0B), (4321:0A0B) \}
\]
are obviously not equivalent by definition, but from the first feedback \fb{0}{0} we could already exclude the colors 1 -- 4 from the potential secret, so any permutation of 1234 should ideally be considered equivalent.

Despite the less-than-ideal properties of such definition, this equivalence relation is still commonly used \cite{neuwirth81,koyama93,francis10} because of its clarity, relative simplicity, and effectiveness for the first few guesses. A more powerful (but also more complex) equivalence definition will be introduced in the next section.

To actually apply a constraint equivalence filter in practice, we implement an \emph{incremental filter}, as described below.\footnote{The incremental filter described in this section is effectively the same technique used by \cite{francis10} to detect equivalence in the Bulls and cows game.}
An alternative, more generic method, which relies on \emph{graph automorphism}, will be introduced in the next section.

Given a set $\mathcal{C}$ of constraints and a set $S$ of candidate codewords for the next guess, we want to find all canonical codewords $g \in S$ to be used as the next guess. We could do this by iterating each $g \in S$ and cross out all codewords $h \in S$ equivalent to $g$. To determine whether two guesses are equivalent, we need to find a permutation $\pi$ that maps one to the other.

We make the assumption that the number of colors is more than the number of pegs, which is typical in a real-world game.

Suppose there are $p$ pegs. We first list all $p!$ permutations of the pegs. For example, in a 4-peg game, there are 24 peg permutations, $\pi_p^1$ to $\pi_p^{24}$, where $\pi_p^1$ is the identity permutation.

Then, for each peg permutation $\pi_p^i$, we associate with it a \emph{partial permutation} of the colors, $\Pi_c^i$. A partial permutation is an incomplete permutation where the mapping of some colors are specified, but the mapping the rest colors are not specified (i.e. they're free to be specified later). Formally, a partial permutation is a set of permutations. [put the definition in the appendix]

At the beginning of the game, there are no constraints, and all associated partial color permutations $\Pi_c^i$ are fully unspecified. We can conveniently write them as
\[
\Pi_c^i = 
\begin{pmatrix}
1 & 2 & 3 & 4 & 5 & 6 \\
* & * & * & * & * & * 
\end{pmatrix} .
\]

To filter a set of codewords $S$ to obtain canonical guesses, we traverse the set in a certain order.\footnote{The particular order is not important; though it is often convenient to traverse in lexicographical order.} 
The next uncrossed codeword must be a canonical guess, thus we choose it as the representative of its equivalence class. Suppose this codeword is \cw{1123}. Then, we iterate through each peg permutation $\pi_p^i$. Take for example the first peg permutation,
\[
\pi_p^1 = 
\begin{pmatrix}
1 & 2 & 3 & 4 \\
1 & 2 & 3 & 4
\end{pmatrix} .
\]
The associated partial color permutation is initially
\[
\Pi_c^1 = 
\begin{pmatrix}
1 & 2 & 3 & 4 & 5 & 6 \\
* & * & * & * & * & * 
\end{pmatrix} .
\]
For the concerned representative, \cw{1123}, we are free to map its colors (1, 2, 3) to any color. We can therefore do a simple iteration to generate all complete color permutations $\pi_c \in \Pi_c^1$. There are $6! / 3! = 120$ such permutations. As a result we get 120 codewords equivalent to this representative (including the representative itself). We cross them out.

We then proceed with the second peg permutation,
\[
\pi_p^2 = 
\begin{pmatrix}
1 & 2 & 3 & 4 \\
1 & 2 & 4 & 3
\end{pmatrix} ,
\] 
%The resulting codeword is now 1243, but this has already been crossed
%out in the above step. Therefore we skip it. Similarly, we skip all
%codewords in this round.
and repeat the above. Continue the same for all peg permutations. After we finish, we are guaranteed to have found (and cross out) all equivalent codewords to this representative, because we have iterated all peg/color permutations and by property [xx] we know that we can reach all elements in the equivalence class in this way. Therefore, we repeat this process for all (uncrossed) codewords to finish the filtering.

Above is an examine where all color permutations are fully unspecified. Now suppose we have already have some constraints. Without loss of generality, suppose that the constraint consists of just one guess $g_1 = \cw{3445}$. To find all canonical candidates for the second guess, we first need to restrict the partial color permutations associated with each peg permutation. 

Take for example 
\[
\pi_p^3 = 
\begin{pmatrix}
1 & 2 & 3 & 4 \\
1 & 3 & 2 & 4
\end{pmatrix} .
\] 
The peg-permuted first guess is $\pi_p^3(g_1) = \cw{3445}$. In order for the permuted constraint to be equivalent, the permuted guess must be equal to $g_1$. This means we must map \cw{3445} to \cw{3445} in the color permutation. Thus we must restrict the associated color permutation as
\[
\Pi_c^3 = 
\begin{pmatrix}
1 & 2 & 3 & 4 & 5 & 6 \\
* & * & 3 & 4 & 5 & * 
\end{pmatrix} .
\]

After restricting the partial permutation, we can apply the previous steps again. Suppose we come again to \cw{1123}. If we apply peg permutation $\pi_p^3$, the permuted representative becomes \cw{1213}. Now to find out what color permutations are eligible, note that the color 3 is already fixed in the partial permutation; only colors 1 and 2 are free to be mapped. We could therefore map them freely among the unrestricted colors, i.e.\ $\{1, 2, 6\}$, and mark all resulting codewords as equivalent to \cw{1123}.

Note that given a set of constraints, not all peg permutations can yield an eligible color permutation. For example, consider the peg permutation
\[
\pi_p^2 = 
\begin{pmatrix}
1 & 2 & 3 & 4 \\
1 & 2 & 4 & 3
\end{pmatrix} .
\]
When applied to $g_1 = \cw{3445}$, we get $\pi_p^2(g_1) = \cw{3454}$. However, there is no way whatsoever to map \cw{3445} to \cw{3454}, because 4 would have to be mapped to both 4 and 5, which is impossible. In this case, we call the peg permutation ($\pi_p^2$) \emph{ineligible} and remove it from further consideration.

Formally, let $\mathcal{F} = \left\{(\pi_p^i, \Pi_c^i) \given 1 \le i \le m \right\}$ be an incremental equivalence filter containing $m$ (partial) codeword permutations. Each time a constraint (with guess $g$) is added, we restrict the partial color permutation by requiring $\pi_c^i ( \pi_p^i (g) ) = \pi_p^i (g)$ for all $\pi_c^i \in \Pi_c^i$. 

It can be seen from the above requirement that any color that is mapped from in a partial color partition is also mapped to, and any color that is not mapped in one direction is also not mapped in the other direction. In fact, the unmapped colors are those \emph{unused} in any of the prior constraints. This means this equivalence filter fully considers the ``unguessed color equivalence'' described in the previous section.

As we proceed in the game, we are supplied with more constraints. The partial color permutation associated with each peg permutation gets more restrictive with each
added constraint, and more peg permutations becomes ineligible and gets removed. Finally we will be left with only the identity codeword permutation, where every codeword will become a representative of its own equivalence class.
%Hence we call it "incremental equivalence detection".

Applying the above constraint equivalence filter, we find the following canonical guesses for the first round of a standard Mastermind game (p4c6r). Listed alongside is the number of canonical guesses in the second round given the initial guess.
\begin{center}
\begin{tabular}{c c}
\hline
\hline
$g_1$ & $\#\{g_2\}$ \\
\hline
\cw{0000} & 12 \\
\cw{0001} & 53 \\
\cw{0011} & 39 \\
\cw{0012} & 130 \\
\cw{0123} & 57 \\
\hline
\hline
\end{tabular}
\end{center}

Applying the same filter to the Bulls and cows game (p4c10n) yields only one canonical initial guess, \cw{0123}, and the following 20 canonical guesses for the second round along with the number of canonical guesses for the third round.\footnote{The table on page 7 of \cite{francis10} contains the same information; however their number for \cw{0456} is 373 and their number for \cw{4567} is 218.}
\begin{center}
\begin{tabular}{c c | c c | c c | c c}
\hline
\hline
$g_2$ & $\#\{g_3\}$ & $g_2$ & $\#\{g_3\}$ & $g_2$ & $\#\{g_3\}$ & $g_2$ & $\#\{g_3\}$ \\
\hline
\cw{0123} & 20  & \cw{0214} & 270  & \cw{1032} & 39  & \cw{1234} & 501  \\
\cw{0124} & 107 & \cw{0231} & 75   & \cw{1034} & 270 & \cw{1245} & 1045 \\
\cw{0132} & 67  & \cw{0234} & 501  & \cw{1045} & 295 & \cw{1435} & 541  \\
\cw{0134} & 270 & \cw{0245} & 1045 & \cw{1204} & 175 & \cw{1456} & 1012 \\
\cw{0145} & 295 & \cw{0456} & 363  & \cw{1230} & 59  & \cw{4567} & 180  \\
\hline
\hline
\end{tabular}
\end{center}

[consistent with all prior guesses]

\subsubsection{Partition equivalence}

State/Partition Equivalence

\subsection{Search space pruning}

\subsection{Other techniques}

(e.g. two-phase optimization, hash collision group)

\subsection{Using a pre-built strategy tree}

\subsection{Extended/Adaptive strategy tree}

i.e. the tree not only contains guesses along the chosen strategy path, but also includes guesses if the user made a non-optimal guess halfway. The tree size in this case is much larger, and we must use isomorphism to detect the symmetry.

 
